{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the third blog post of [Object Detection with YOLO blog series](https://fairyonice.github.io/tag/object-detection-using-yolov2-on-pascal-voc2012-series.html). This blog discusses the YOLO's model architecture and loss funciton. I will use PASCAL VOC2012 data. This blog assumes that the readers have read the previous two blog posts - [Part 1](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html), [Part 2](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html).\n",
    "\n",
    "\n",
    "## Andrew Ng's Yolo lecture\n",
    "- [Neural Networks - Bounding Box Predictions](https://www.youtube.com/watch?v=gKreZOUi-O0&t=0s&index=7&list=PL_IHmaMAvkVxdDOBRg2CbcJBq9SY7ZUvs)\n",
    "- [C4W3L06 Intersection Over Union](https://www.youtube.com/watch?v=ANIzQ5G-XPE&t=7s)\n",
    "- [C4W3L07 Nonmax Suppression](https://www.youtube.com/watch?v=VAo84c1hQX8&t=192s)\n",
    "- [C4W3L08 Anchor Boxes](https://www.youtube.com/watch?v=RTlwl2bv0Tg&t=28s)\n",
    "- [C4W3L09 YOLO Algorithm](https://www.youtube.com/watch?v=9s_FpMpdYW8&t=34s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reference\n",
    "- [You Only Look Once:Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640.pdf) \n",
    "\n",
    "- [YOLO9000:Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242.pdf)\n",
    " \n",
    "- [experiencor/keras-yolo2](https://github.com/experiencor/keras-yolo2)\n",
    "\n",
    "## Reference in my blog\n",
    "- [Part 1 Object Detection using YOLOv2 on Pascal VOC2012 - anchor box clustering](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html)\n",
    "\n",
    "- [Part 2 Object Detection using YOLOv2 on Pascal VOC2012 - input and output encoding](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html)\n",
    "\n",
    "- [Part 3 Object Detection using YOLOv2 on Pascal VOC2012 - model and loss](https://fairyonice.github.io/Part_3_Object_Detection_with_Yolo_using_VOC_2014_data_model_and_loss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define anchor box\n",
    "<code>ANCHORS</code> defines the number of anchor boxes and the shape of each anchor box.\n",
    "The choice of the anchor box specialization is already discussed in [Part 1 Object Detection using YOLOv2 on Pascal VOC2012 - anchor box clustering](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html). \n",
    "\n",
    "Based on the K-means analysis in the previous blog post, I will select 4 anchor boxes of following width and height. The width and heights are rescaled in the grid cell scale (Assuming that the number of grid size is 13 by 13.) See [Part 2 Object Detection using YOLOv2 on Pascal VOC2012 - input and output encoding](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html) to learn how I rescal the anchor box shapes into the grid cell scale.\n",
    "\n",
    "Here I choose 4 anchor boxes. With 13 by 13 grids, every frame gets 4 x 13 x 13 = 676 bouding box predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ANCHORS = np.array([1.07709888,  1.78171903,  # anchor box 1, width , height\n",
    "                    2.71054693,  5.12469308,  # anchor box 2, width,  height\n",
    "                   10.47181473, 10.09646365,  # anchor box 3, width,  height\n",
    "                    5.48531347,  8.11011331]) # anchor box 4, width,  height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Label vector containing 20 object classe names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LABELS = ['aeroplane',  'bicycle', 'bird',  'boat',      'bottle', \n",
    "#          'bus',        'car',      'cat',  'chair',     'cow',\n",
    "#          'diningtable','dog',    'horse',  'motorbike', 'person',\n",
    "#          'pottedplant','sheep',  'sofa',   'train',   'tvmonitor']\n",
    "LABELS = ['chair',    'person' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv2 Model Architecture\n",
    "While YOLO's input and output encodings are complex, and loss function of YOLO is quite complex (which will be discussed very soon), the model architecture is simple. \n",
    "It repeatedly stacks Convolusion + Batch Normalization + Leaky Relu layers until the image shape reduces to the grid cell size. \n",
    "Here is the model defenition, extracted from [experiencor/keras-yolo2](https://github.com/experiencor/keras-yolo2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        (None, 416, 416, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 416, 416, 32) 864         input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "norm_1 (BatchNormalization)     (None, 416, 416, 32) 128         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_173 (LeakyReLU)     (None, 416, 416, 32) 0           norm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1_416to208 (MaxPooling2D (None, 208, 208, 32) 0           leaky_re_lu_173[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 208, 208, 64) 18432       maxpool1_416to208[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "norm_2 (BatchNormalization)     (None, 208, 208, 64) 256         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_174 (LeakyReLU)     (None, 208, 208, 64) 0           norm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1_208to104 (MaxPooling2D (None, 104, 104, 64) 0           leaky_re_lu_174[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 104, 104, 128 73728       maxpool1_208to104[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "norm_3 (BatchNormalization)     (None, 104, 104, 128 512         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_175 (LeakyReLU)     (None, 104, 104, 128 0           norm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 104, 104, 64) 8192        leaky_re_lu_175[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_4 (BatchNormalization)     (None, 104, 104, 64) 256         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_176 (LeakyReLU)     (None, 104, 104, 64) 0           norm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 104, 104, 128 73728       leaky_re_lu_176[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_5 (BatchNormalization)     (None, 104, 104, 128 512         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_177 (LeakyReLU)     (None, 104, 104, 128 0           norm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1_104to52 (MaxPooling2D) (None, 52, 52, 128)  0           leaky_re_lu_177[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, 52, 52, 256)  294912      maxpool1_104to52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "norm_6 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_178 (LeakyReLU)     (None, 52, 52, 256)  0           norm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, 52, 52, 128)  32768       leaky_re_lu_178[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_7 (BatchNormalization)     (None, 52, 52, 128)  512         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_179 (LeakyReLU)     (None, 52, 52, 128)  0           norm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, 52, 52, 256)  294912      leaky_re_lu_179[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_8 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_180 (LeakyReLU)     (None, 52, 52, 256)  0           norm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1_52to26 (MaxPooling2D)  (None, 26, 26, 256)  0           leaky_re_lu_180[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, 26, 26, 512)  1179648     maxpool1_52to26[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_9 (BatchNormalization)     (None, 26, 26, 512)  2048        conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_181 (LeakyReLU)     (None, 26, 26, 512)  0           norm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_181[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_10 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_182 (LeakyReLU)     (None, 26, 26, 256)  0           norm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_11 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_182[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_11 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_183 (LeakyReLU)     (None, 26, 26, 512)  0           norm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_12 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_183[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_12 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_184 (LeakyReLU)     (None, 26, 26, 256)  0           norm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_13 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_184[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_13 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_185 (LeakyReLU)     (None, 26, 26, 512)  0           norm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1_26to13 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_185[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_14 (Conv2D)                (None, 13, 13, 1024) 4718592     maxpool1_26to13[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_14 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_186 (LeakyReLU)     (None, 13, 13, 1024) 0           norm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_15 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_186[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_15 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_187 (LeakyReLU)     (None, 13, 13, 512)  0           norm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_187[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_16 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_188 (LeakyReLU)     (None, 13, 13, 1024) 0           norm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_188[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_17 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_189 (LeakyReLU)     (None, 13, 13, 512)  0           norm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_18 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_189[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_18 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_190 (LeakyReLU)     (None, 13, 13, 1024) 0           norm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_190[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_19 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_21 (Conv2D)                (None, 26, 26, 64)   32768       leaky_re_lu_185[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_191 (LeakyReLU)     (None, 13, 13, 1024) 0           norm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_21 (BatchNormalization)    (None, 26, 26, 64)   256         conv_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_191[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_193 (LeakyReLU)     (None, 26, 26, 64)   0           norm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_20 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 13, 13, 256)  0           leaky_re_lu_193[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_192 (LeakyReLU)     (None, 13, 13, 1024) 0           norm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 13, 13, 1280) 0           lambda_8[0][0]                   \n",
      "                                                                 leaky_re_lu_192[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 13, 13, 1024) 11796480    concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_22 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_194 (LeakyReLU)     (None, 13, 13, 1024) 0           norm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 13, 13, 28)   28700       leaky_re_lu_194[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_output (Reshape)          (None, 13, 13, 4, 7) 0           conv_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_hack (InputLayer)         (None, 1, 1, 1, 50,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hack_layer (Lambda)             (None, 13, 13, 4, 7) 0           final_output[0][0]               \n",
      "                                                                 input_hack[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 50,576,636\n",
      "Trainable params: 28,700\n",
      "Non-trainable params: 50,547,936\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# the function to implement the orgnization layer (thanks to github.com/allanzelener/YAD2K)\n",
    "def space_to_depth_x2(x):\n",
    "    return tf.space_to_depth(x, block_size=2)\n",
    "\n",
    "\n",
    "def define_YOLOv2(IMAGE_H,IMAGE_W,GRID_H,GRID_W,TRUE_BOX_BUFFER,BOX,CLASS, trainable=False):\n",
    "    input_image = Input(shape=(IMAGE_H, IMAGE_W, 3),name=\"input_image\")\n",
    "    true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4),name=\"input_hack\")\n",
    "\n",
    "    # Layer 1\n",
    "    x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False, trainable=trainable)(input_image)\n",
    "    x = BatchNormalization(name='norm_1', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # strides = None, strides = pool_size.\n",
    "    x = MaxPooling2D(pool_size=(2, 2),name=\"maxpool1_416to208\")(x)\n",
    "\n",
    "    # Layer 2\n",
    "    x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_2', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2),name=\"maxpool1_208to104\")(x)\n",
    "\n",
    "    # Layer 3\n",
    "    x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_3', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 4\n",
    "    x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_4', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 5\n",
    "    x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_5', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2),name=\"maxpool1_104to52\")(x)\n",
    "\n",
    "    # Layer 6\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_6', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 7\n",
    "    x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_7', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 8\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_8', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2),name=\"maxpool1_52to26\")(x) \n",
    "\n",
    "    # Layer 9\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_9', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 10\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_10', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 11\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_11', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 12\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_12', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 13\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_13', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    skip_connection = x\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2),name=\"maxpool1_26to13\")(x)\n",
    "\n",
    "    # Layer 14\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_14', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 15\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_15', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 16\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_16', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 17\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_17', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 18\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_18', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 19\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_19', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 20\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_20', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 21\n",
    "    skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False, trainable=trainable)(skip_connection)\n",
    "    skip_connection = BatchNormalization(name='norm_21', trainable=trainable)(skip_connection)\n",
    "    skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
    "    skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
    "\n",
    "    x = concatenate([skip_connection, x])\n",
    "\n",
    "    # Layer 22\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(name='norm_22', trainable=trainable)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 23\n",
    "    x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
    "    output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS),name=\"final_output\")(x)\n",
    "\n",
    "    # small hack to allow true_boxes to be registered when Keras build the model \n",
    "    # for more information: https://github.com/fchollet/keras/issues/2790\n",
    "    output = Lambda(lambda args: args[0],name=\"hack_layer\")([output, true_boxes])\n",
    "\n",
    "    model = Model([input_image, true_boxes], output)\n",
    "    return(model, true_boxes)\n",
    "\n",
    "IMAGE_H, IMAGE_W  = 416, 416\n",
    "GRID_H,  GRID_W   = 13 , 13\n",
    "TRUE_BOX_BUFFER   = 50\n",
    "BOX               = int(len(ANCHORS)/2)\n",
    "CLASS             = len(LABELS)\n",
    "## true_boxes is the tensor that takes \"b_batch\"\n",
    "model, true_boxes = define_YOLOv2(IMAGE_H,IMAGE_W,GRID_H,GRID_W,TRUE_BOX_BUFFER,BOX,CLASS, \n",
    "                                  trainable=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained YOLOv2 weights \n",
    "Following the instruction at [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolov2/), we download the pre-trained weights using wget: \n",
    "\n",
    "<code>\n",
    "wget https://pjreddie.com/media/files/yolov2.weights\n",
    "</code>\n",
    "\n",
    "The weights are saved at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_weight = \"./yolov2.weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes are extracted from [keras-yolo2/Yolo Step-by-Step.ipynb](https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_weights.shape = (50983565,)\n"
     ]
    }
   ],
   "source": [
    "class WeightReader:\n",
    "    # code from https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb\n",
    "    def __init__(self, weight_file):\n",
    "        self.offset = 4\n",
    "        self.all_weights = np.fromfile(weight_file, dtype='float32')\n",
    "        \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.offset = 4\n",
    "                \n",
    "weight_reader = WeightReader(path_to_weight)\n",
    "print(\"all_weights.shape = {}\".format(weight_reader.all_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign pre-trained weights to the following layers: \n",
    "<code> conv_i</code>, <code>norm_i</code>, <code> i = 1, 2,..., 22</code>. \n",
    "These layers do not depend on the number of object classes or the number of anchor boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_reader.reset()\n",
    "nb_conv = 22\n",
    "\n",
    "for i in range(1, nb_conv+1):\n",
    "    conv_layer = model.get_layer('conv_' + str(i)) ## convolusional layer\n",
    "    \n",
    "    if i < nb_conv:\n",
    "        norm_layer = model.get_layer('norm_' + str(i)) ## batch normalization layer\n",
    "        \n",
    "        size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "\n",
    "        beta  = weight_reader.read_bytes(size)\n",
    "        gamma = weight_reader.read_bytes(size)\n",
    "        mean  = weight_reader.read_bytes(size)\n",
    "        var   = weight_reader.read_bytes(size)\n",
    "\n",
    "        weights = norm_layer.set_weights([gamma, beta, mean, var])       \n",
    "        \n",
    "    if len(conv_layer.get_weights()) > 1: ## with bias\n",
    "        bias   = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "        kernel = kernel.transpose([2,3,1,0])\n",
    "        conv_layer.set_weights([kernel, bias])\n",
    "    else: ## without bias\n",
    "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "        kernel = kernel.transpose([2,3,1,0])\n",
    "        conv_layer.set_weights([kernel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating loss function of Yolo v2\n",
    "There has been a lot of discussion on understanding YOLO loss funciton on line.\n",
    "For example, at [Understanding YOLO](https://hackernoon.com/understanding-yolo-f5a74bbc7967).\n",
    "However, most of these posts discusses the loss function of Yolo v1 which must be different from Yolo v2.\n",
    "The two losses are different and the lack of explicit formula for the Yolo v2 loss paper arises some confusion, for example at [What is YOLOv2 Loss Function - Google Groups](https://groups.google.com/forum/#!topic/darknet/TJ4dN9R4iJk).\n",
    "\n",
    "\n",
    "The YOLO v1 is difined in \n",
    "[You Only Look Once:Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640.pdf) as:\n",
    "### YOLO V1 loss \n",
    "$$\\begin{multline}\n",
    "\\lambda_\\textbf{coord}\n",
    "\\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "     L_{ij}^{\\text{obj}}\n",
    "            \\left[\n",
    "            \\left(\n",
    "                x_i - \\hat{x}_i\n",
    "            \\right)^2 +\n",
    "            \\left(\n",
    "                y_i - \\hat{y}_i\n",
    "            \\right)^2\n",
    "            \\right]\n",
    "\\\\\n",
    "+ \\lambda_\\textbf{coord} \n",
    "\\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "         L_{ij}^{\\text{obj}}\n",
    "         \\left[\n",
    "        \\left(\n",
    "            \\sqrt{w_i} - \\sqrt{\\hat{w}_i}\n",
    "        \\right)^2 +\n",
    "        \\left(\n",
    "            \\sqrt{h_i} - \\sqrt{\\hat{h}_i}\n",
    "        \\right)^2\n",
    "        \\right]\n",
    "\\\\\n",
    "+ \\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "        L_{ij}^{\\text{obj}}\n",
    "        \\left(\n",
    "            C_i - \\hat{C}_i\n",
    "        \\right)^2\n",
    "\\\\\n",
    "+ \\lambda_\\textrm{noobj}\n",
    "\\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "    L_{ij}^{\\text{noobj}}\n",
    "        \\left(\n",
    "            C_i - \\hat{C}_i\n",
    "        \\right)^2\n",
    "\\\\\n",
    "+ \\sum_{i = 0}^{S^2}\n",
    "L_i^{\\text{obj}}\n",
    "    \\sum_{c \\in \\textrm{classes}}\n",
    "        \\left(\n",
    "            p_i(c) - \\hat{p}_i(c)\n",
    "        \\right)^2\n",
    "\\end{multline}$$\n",
    "\n",
    "YOLOv2 paper expalins the difference in architecture from YOLOv1 as follows:\n",
    "\n",
    "<blockquote>\n",
    "We remove the fully connected layers from YOLO(v1) and use anchor boxes to predict bounding boxes...\n",
    "When we move to anchor boxes we also decouple the class prediction mechanism from the spatial location and instead predict class and objectness for every anchorbox. \n",
    "</blockquote>\n",
    "This means that probability $p_i(c)$ should depend not only on $i$ and $c$ but also $j$ i.e., anchor box index.\n",
    "\n",
    "\n",
    "The output of the loss functions \n",
    "- [ $x$, $y$, $w$, $h$, $C$, $P_{C_1}$,$P_{C_2}$, .. $P_{C_{20}}$]\n",
    "\n",
    "The loss function of Yolo treats each of these output entries separately. \n",
    "So it looks complicated at the first grance. \n",
    "So let's understand each term of the loss one by one.\n",
    "\n",
    "$$\n",
    "L_{i,j} = L_{i,j}^{xywh} + L_{i,j}^c + L_{i,j}^p\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "L_{i,j}^{xywh}&=\n",
    "\\lambda_{\\textrm{coord}}\n",
    "\\sum_{i=0}^{S^2}\n",
    "\\sum_{j=0}^B\n",
    "L_{i,j}^{\\text{obj}}\n",
    "\\left[\n",
    "\\left(x_{i,j}-\\hat{x}_{i,j}\\right)^2 + \n",
    "\\left(y_{i,j}-\\hat{y}_{i,j}\\right)^2 +\n",
    "\\left(\\sqrt{w}_{i,j}-\\sqrt{\\hat{w}}_{i,j}\\right)^2 +\n",
    "\\left(\\sqrt{h}_{i,j}-\\sqrt{\\hat{h}}_{i,j}\\right)^2 \n",
    "\\right]\\\\\n",
    "L_{i,j}^c &=\n",
    "\\sum_{i=0}^{S^2}\n",
    "\\sum_{j=0}^B\n",
    "L_{i,j}^{\\text{obj}}\n",
    "\\left(\n",
    "IOU_{\\text{preduiction}_{i,j}}^{\\text{ground truth}_{i,j}} - \\widehat{C}_{i,j}\n",
    "\\right)^2\n",
    "+\n",
    "\\lambda_{\\textrm{noobj}}\n",
    "\\sum_{i=0}^{S^2}\n",
    "\\sum_{j=0}^B\n",
    "L_{i,j}^{\\text{noobj}}\n",
    "\\left(0 - \\widehat{C}_{i,j}\\right)\\\\\n",
    "L_{i,j}^p&=-\\sum_{i=0}^{S^2} \\sum_{j=0}^B L_{i,j}^{\\text{obj}}\\sum_{c \\in \\text{class}} p_{i,j}^c \\text{log}(\\hat{p}_{i,j})\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "L_{i,j}^{\\text{noobj}}\n",
    "& = \\texttt{conf-mask} = \n",
    "\\begin{cases}\n",
    " 1 \\;\\;\\text{if}\\;\\;\\text{max}_{i',j'}\n",
    " \\;\\;IOU_{\\text{preduiction}_{i,j}}^{\\text{ground truth}_{i',j'}} < 0.6 \\;\\text{and}\\; C_{i,j} = 0\\\\\n",
    " 0\\;\\;\\text{else}\\\\\n",
    "\\end{cases}\\\\\n",
    "L_{i,j}^{\\text{obj}}\n",
    " &= \\texttt{coord-mask} = \n",
    " \\begin{cases}\n",
    " 1 \n",
    "\\;\\;\\text{if} \\;\\;C_{i,j}=1\\\\\n",
    " 0\\;\\;\\text{else}\\\\\n",
    "\\end{cases}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Assume the following parameters are defined:\n",
    "# - BATCH_SIZE and \n",
    "# - BOX \n",
    "# - CLASS_WEIGHTS\n",
    "# - NO_OBJECT_SCALE\n",
    "# - OBJECT_SCALE\n",
    "# - COORD_SCALE\n",
    "# - CLASS_SCALE\n",
    "# - WEIGHT\n",
    "\n",
    "BATCH_SIZE       = 16\n",
    "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
    "NO_OBJECT_SCALE  = 1.0\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "WARM_UP_BATCHES  = 0\n",
    "\n",
    "def get_cell_grid(): \n",
    "    '''\n",
    "    (batch size, GRID_H, GRID_W, 3, 2)\n",
    "    output\n",
    "    for any i=0,1..,batch size - 1\n",
    "    output[i,5,3,:,:] = array([[3., 5.],\n",
    "                               [3., 5.],\n",
    "                               [3., 5.]], dtype=float32)\n",
    "    '''\n",
    "    ## cell_x.shape = (1, 13, 13, 1, 1)\n",
    "    ## cell_x[:,i,j,:] = [[[j]]]\n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
    "    ## cell_y.shape = (1, 13, 13, 1, 1)\n",
    "    ## cell_y[:,i,j,:] = [[[i]]]\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "    ## cell_gird.shape = (16, 13, 13, 5, 2)\n",
    "    ## for any n, k, i, j\n",
    "    ##    cell_grid[n, i, j, anchor, k] = j when k = 0\n",
    "    ## for any n, k, i, j\n",
    "    ##    cell_grid[n, i, j, anchor, k] = i when k = 1    \n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, BOX, 1])\n",
    "    return(cell_grid) \n",
    "def adjust_scale_prediction(y_pred, cell_grid):    \n",
    "    \"\"\"\n",
    "        Adjust prediction\n",
    "        \n",
    "        == input ==\n",
    "        \n",
    "        y_pred : takes any real values\n",
    "                 tensor of shape = (N batch, NGridy, NGridx, NAnchor, 4 + 1 + N class)\n",
    "        \n",
    "        == output ==\n",
    "        \n",
    "        pred_box_xy : shape = (N batch, N grid x, N grid y, N anchor, 2), contianing [center_y, center_x] rangining [0,0]x[grid_H-1,grid_W-1]\n",
    "          pred_box_xy[irow,igrid_h,igrid_w,ianchor,0] =  center_x\n",
    "          pred_box_xy[irow,igrid_h,igrid_w,ianchor,1] =  center_1\n",
    "          \n",
    "          calculation process:\n",
    "          tf.sigmoid(y_pred[...,:2]) : takes values between 0 and 1\n",
    "          tf.sigmoid(y_pred[...,:2]) + cell_grid : takes values between 0 and grid_W - 1 for x coordinate \n",
    "                                                   takes values between 0 and grid_H - 1 for y coordinate \n",
    "                                                   \n",
    "        pred_Box_wh : shape = (N batch, N grid x, N grid y, N anchor, 2), containing width and height, rangining [0,0]x[grid_H-1,grid_W-1]\n",
    "        \n",
    "        pred_box_conf : shape = (N batch, N grid x, N grid y, N anchor, 1), containing confidence to range between 0 and 1\n",
    "        \n",
    "        pred_box_class : shape = (N batch, N grid x, N grid y, N anchor, N class), containing \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ## cell_grid is of the shape of \n",
    "    \n",
    "    ### adjust x and y  \n",
    "    # the bounding box bx and by are rescaled to range between 0 and 1 for given gird.\n",
    "    # Since there are BOX x BOX grids, we rescale each bx and by to range between 0 to BOX + 1\n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid # bx, by\n",
    "    \n",
    "    ### adjust w and h\n",
    "    # exp to make width and height positive\n",
    "    # rescale each grid to make some anchor \"good\" at representing certain shape of bounding box \n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS,[1,1,1,BOX,2]) # bw, bh\n",
    "\n",
    "    ### adjust confidence \n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])# prob bb\n",
    "\n",
    "    ### adjust class probabilities \n",
    "    pred_box_class = y_pred[..., 5:] # prC1, prC2, ..., prC20\n",
    "    \n",
    "    return(pred_box_xy,pred_box_wh,pred_box_conf,pred_box_class)\n",
    "\n",
    "\n",
    "def get_intersect_area(true_xy,true_wh,\n",
    "                      pred_xy,pred_wh):\n",
    "    '''\n",
    "    == INPUT ==\n",
    "    true_xy,pred_xy, true_wh and pred_wh must have the same shape length\n",
    "\n",
    "    p1 : pred_mins = (px1,py1)\n",
    "    p2 : pred_maxs = (px2,py2)\n",
    "    t1 : true_mins = (tx1,ty1) \n",
    "    t2 : true_maxs = (tx2,ty2) \n",
    "                 p1______________________ \n",
    "                 |      t1___________   |\n",
    "                 |       |           |  |\n",
    "                 |_______|___________|__|p2 \n",
    "                         |           |rmax\n",
    "                         |___________|\n",
    "                                      t2\n",
    "    intersect_mins : rmin = t1  = (tx1,ty1)\n",
    "    intersect_maxs : rmax = (rmaxx,rmaxy)\n",
    "    intersect_wh   : (rmaxx - tx1, rmaxy - ty1)\n",
    "        \n",
    "    '''\n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "    \n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)    \n",
    "    return(iou_scores)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    '''\n",
    "    y_true : (N batch, N grid h, N grid w, N anchor, 4 + 1 + N classes)\n",
    "    y_true[irow, i_gridh, i_gridw, i_anchor, :4] = center_x, center_y, w, h\n",
    "    \n",
    "        center_x : The x coordinate center of the bounding box.\n",
    "                   Rescaled to range between 0 and N gird  w (e.g., ranging between [0,13)\n",
    "        center_y : The y coordinate center of the bounding box.\n",
    "                   Rescaled to range between 0 and N gird  h (e.g., ranging between [0,13)\n",
    "        w        : The width of the bounding box.\n",
    "                   Rescaled to range between 0 and N gird  w (e.g., ranging between [0,13)\n",
    "        h        : The height of the bounding box.\n",
    "                   Rescaled to range between 0 and N gird  h (e.g., ranging between [0,13)\n",
    "                   \n",
    "    y_true[irow, i_gridh, i_gridw, i_anchor, 4] = ground truth confidence\n",
    "        \n",
    "        ground truth confidence is 1 if object exists in this (anchor box, gird cell) pair\n",
    "    \n",
    "    y_true[irow, i_gridh, i_gridw, i_anchor, 5 + iclass] = 1 if the object is in category <iclass> else 0\n",
    "        \n",
    "    '''\n",
    "\n",
    "    \n",
    "    seen = tf.Variable(0.)\n",
    "    total_recall = tf.Variable(0.)\n",
    "    \n",
    "    cell_grid = get_cell_grid()\n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    pred_box_xy, pred_box_wh, pred_box_conf, pred_box_class = adjust_scale_prediction(y_pred,cell_grid)\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "    \n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "    \n",
    "    iou_scores  =  get_intersect_area(true_box_xy,true_box_wh,\n",
    "                                      pred_box_xy,pred_box_wh)\n",
    "\n",
    "    # true_box_conf value depends on the predicted values \n",
    "    # true_box_conf = IOU_{true,pred} if objecte exist in this anchor else 0\n",
    "    true_box_conf = iou_scores * y_true[..., 4]\n",
    "    \n",
    "    ### adjust class probabilities (N batch, N grid h , N grid x ,N anchor)\n",
    "    # return index of the object class\n",
    "    # what would happen if there is no object? true_box_class is recorded as zero.\n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Determine the masks\n",
    "    \"\"\"\n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    ###                  this will serve as lambda_{coord} 1_{i,j}^{obj}\n",
    "    ### (grid_cell, anchor box) pair that object receives 1 else 0\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE ## (16,13,13,5,1)\n",
    "    \n",
    "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    true_xy = true_boxes[..., 0:2]           # (N batch, 1, 1, 1, TRUE_BOX_BUFFER, 2)\n",
    "    true_wh = true_boxes[..., 2:4]           # (N batch, 1, 1, 1, TRUE_BOX_BUFFER, 2)\n",
    "    \n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4) # (N batch, N grid_h, N grid_w, N anchor, 1, 2)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4) # (N batch, N grid_h, N grid_w, N anchor, 1, 2)\n",
    "    \n",
    "    iou_scores  =  get_intersect_area(true_xy,true_wh,\n",
    "                                      pred_xy,pred_wh)# (16, 13, 13, 5, 50)    \n",
    "    \n",
    "    # for each iframe,\n",
    "    # best_ious[iframe,igridy,igridx,ianchor] contains\n",
    "    #    the IOU of the object that is most likely included (or best fitted) within the bounded box recorded in (grid_cell, anchor) pair\n",
    "    #    NOTE: a same object may be contained in multiple (grid_cell, anchor) pair\n",
    "    ##         from best_ious, you cannot tell how may actual objects are captured as the \"best\" object\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4) # (16, 13, 13, 5)\n",
    "    # conf_mask[iframe, igridy, igridx, ianchor] = 0\n",
    "    #           when there is no object in (grid cell, anchor) pair and the region seems useless oi.e. \n",
    "    #           y_true[iframe,igridx,igridy,4] = 0 \"and\" the predicted region has no object that has IoU > 0.6\n",
    "    # conf_mask[iframe, igridy, igridx, ianchor] =  NO_OBJECT_SCALE\n",
    "    #           when there is no object in (grid cell, anchor) pair but region seems to include some object\n",
    "    #           y_true[iframe,igridx,igridy,4] = 0 \"and\" the predicted region has some object that has IoU > 0.6\n",
    "    # conf_mask[iframe, igridy, igridx, ianchor] =  OBJECT_SCALE\n",
    "    #           when there is an object in (grid cell, anchor) pair    \n",
    "    conf_mask = tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
    "    \n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
    "    \n",
    "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
    "    ## tf.gather: Gather slices from params axis according to indices.\n",
    "    ## CLASS_WEIGHTS = [1]*Nclass and CLASS+SCALE = 1.0\n",
    "    ## then class_mask[iframe,igridy,igridx,ianchor] = 1 if object exists in this (grid_cell, anchor) pair and 0 otherwise.\n",
    "    ## \n",
    "    class_mask = y_true[..., 4] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE       \n",
    "    \n",
    "    \"\"\"\n",
    "    Warm-up training\n",
    "    \"\"\"\n",
    "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
    "    seen = tf.assign_add(seen, 1.)\n",
    "    \n",
    "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
    "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
    "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, \n",
    "                                   tf.ones_like(coord_mask)],\n",
    "                          lambda: [true_box_xy, \n",
    "                                   true_box_wh,\n",
    "                                   coord_mask])\n",
    "    \n",
    "    \"\"\"\n",
    "    Finalize the loss\n",
    "    \"\"\"\n",
    "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
    "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "    \n",
    "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
    "    \n",
    "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
    "    \n",
    "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "\n",
    "    \"\"\"\n",
    "    Debugging code\n",
    "    \"\"\"    \n",
    "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
    "    total_recall = tf.assign_add(total_recall, current_recall) \n",
    "\n",
    "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The location where the VOC2012 data is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image_folder = \"../ObjectDetectionRCNN/VOCdevkit/VOC2012/JPEGImages/\"\n",
    "train_annot_folder = \"../ObjectDetectionRCNN/VOCdevkit/VOC2012/Annotations/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images and annotations into memory\n",
    "Use the pre-processing code for parsing annotation at [experiencor/keras-yolo2](https://github.com/experiencor/keras-yolo2).\n",
    "This <code>parse_annoation</code> function is already used in [Part 1 Object Detection using YOLOv2 on Pascal VOC2012 - anchor box clustering](https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html) and saved in my python script. \n",
    "This script can be downloaded at [my Github repository, FairyOnIce/ObjectDetectionYolo/backend](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train = 10369\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "from backend import parse_annotation\n",
    "train_image, seen_train_labels = parse_annotation(train_annot_folder,\n",
    "                                                  train_image_folder, \n",
    "                                                  labels=LABELS)\n",
    "print(\"N train = {}\".format(len(train_image)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate batch generator object\n",
    "<code>SimpleBatchGenerator</code> is discussed and used in \n",
    "[Part 2 Object Detection using YOLOv2 on Pascal VOC2012 - input and output encoding](https://fairyonice.github.io/Part%202_Object_Detection_with_Yolo_using_VOC_2014_data_input_and_output_encoding.html).\n",
    "This script can be downloaded at [my Github repository, FairyOnIce/ObjectDetectionYolo/backend](https://github.com/FairyOnIce/ObjectDetectionYolo/blob/master/backend.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from backend import SimpleBatchGenerator\n",
    "\n",
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, \n",
    "    'IMAGE_W'         : IMAGE_W,\n",
    "    'GRID_H'          : GRID_H,  \n",
    "    'GRID_W'          : GRID_W,\n",
    "    'LABELS'          : LABELS,\n",
    "    'ANCHORS'         : ANCHORS,\n",
    "    'BATCH_SIZE'      : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : TRUE_BOX_BUFFER,\n",
    "}\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    return image / 255.\n",
    "train_batch_generator = SimpleBatchGenerator(train_image, generator_config,\n",
    "                                             norm=normalize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training starts here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 18/649 [..............................] - ETA: 2:16:24 - loss: 55.0035"
     ]
    }
   ],
   "source": [
    "dir_log = \"logs/\"\n",
    "try:\n",
    "    os.makedirs(dir_log)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           min_delta=0.001, \n",
    "                           patience=3, \n",
    "                           mode='min', \n",
    "                           verbose=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights_coco.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             period=1)\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
    "#optimizer = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss=custom_loss, optimizer=optimizer)\n",
    "\n",
    "model.fit_generator(generator        = train_batch_generator, \n",
    "                    steps_per_epoch  = len(train_batch_generator), \n",
    "                    epochs           = 5, \n",
    "                    verbose          = 1,\n",
    "                    #validation_data  = valid_batch,\n",
    "                    #validation_steps = len(valid_batch),\n",
    "                    callbacks        = [early_stop, checkpoint], \n",
    "                    max_queue_size   = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
